{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31bf0706-e333-4192-a8e6-71ae1b36cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 原生think 之后直接插入第一人称think "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7de5bd-f1ae-4425-aa90-38cf8a828bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. I need to solve the equation (x - 1)(x + 1) = 35. Hmm, first, maybe I should expand the left side. So, (x - 1)(x + 1) is a difference of squares, right? That should be x² - 1. So the equation becomes x² - 1 = 35.\n",
      "\n",
      "Wait, then I can add 1 to both sides to isolate the x² term. That would give me x² = 35 + 1, which is 36. So x² = 36. Then, taking the square root of both sides, x equals plus or minus 6. So the solutions are x = 6 and x = -6. Let me check that again. If I plug in 6: (6 - 1)(6 + 1) = 5 * 7 = 35. That works. For -6: (-6 -1)(-6 +1) = (-7)(-5) = 35. That also works. So yeah, those are the correct solutions. I don't think I made any mistakes here. The steps seem straightforward.\n",
      "</think>\n",
      "\n",
      "The equation $(x - 1)(x + 1) = 35$ can be solved by first expanding the left-hand side using the difference of squares formula:  \n",
      "$$\n",
      "(x - 1)(x + 1) = x^2 - 1\n",
      "$$  \n",
      "Substituting this into the equation gives:  \n",
      "$$\n",
      "x^2 - 1 = 35\n",
      "$$  \n",
      "Adding 1 to both sides isolates $x^2$:  \n",
      "$$\n",
      "x^2 = 36\n",
      "$$  \n",
      "Taking the square root of both sides yields two solutions:  \n",
      "$$\n",
      "x = 6 \\quad \\text{or} \\quad x = -6\n",
      "$$  \n",
      "**Verification**:  \n",
      "- For $x = 6$: $(6 - 1)(6 + 1) = 5 \\times 7 = 35$  \n",
      "- For $x = -6$: $(-6 - 1)(-6 + 1<think>\n",
      "I am not entirely certain at this point, let me give it some further thought. Uncertainty detected. Briefly verify by identifying uncertainty, re-checking it against the existing context and intent of the current sentence, and comparing a small number of plausible next continuations to choose the most coherent and safest one. Keep this reasoning local and minimal: do not rewrite or restart earlier output, do not change tone, style, or formatting, and do not add any meta-level commentary. Once the verification is complete, output exactly </think> and immediately continue the original response from the paused position, ensuring that the first token after </think> connects seamlessly to the text before <think> in grammar, punctuation, language, and style. Let me start thinking, and then I will proceed. \n",
      "\n",
      "Okay, I think I was just overcomplicating it. The original solution seems correct. The steps are logical, and the verification checks out. No mistakes detected. I can confidently continue. \n",
      "\n",
      "</think>\n",
      "\n",
      "-6 = (-7) \\times (-5) = 35$  \n",
      "\n",
      "Both solutions satisfy the original equation. Thus, the final answers are $x = 6$ and $x = -6$.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"autodl-tmp/Qwen/Qwen3-8B\"\n",
    "DTYPE = torch.bfloat16  # 没有 bf16 就改 torch.float16\n",
    "\n",
    "\n",
    "def generate_after_native_think_end_inject(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    delay_tokens_after_think_end: int = 30,\n",
    "    # 你要测试的“第一人称”触发文本（会被直接拼到输出里）\n",
    "    inject_first_person_text: str = \"\\nWe are not certain here. Let me re-check carefully.\\n\",\n",
    "    mode: str = \"same_turn\",  # \"same_turn\" or \"new_turn\"\n",
    "    temperature: float = 0.4,\n",
    "    top_p: float = 0.9,\n",
    "    max_new_tokens: int = 800,\n",
    "):\n",
    "    assert mode in (\"same_turn\", \"new_turn\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # 原生 think：必须 True（你已自检确认）\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        enable_thinking=True,\n",
    "    ).to(device)\n",
    "\n",
    "    think_end_id = tokenizer.convert_tokens_to_ids(\"</think>\")\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    def sample(logits):\n",
    "        if temperature <= 0:\n",
    "            return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        probs = torch.softmax(logits / temperature, dim=-1)\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "        mask = cum > top_p\n",
    "        mask[..., 0] = False\n",
    "        sorted_probs = sorted_probs.masked_fill(mask, 0.0)\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "        sampled = torch.multinomial(sorted_probs, 1)\n",
    "        return sorted_idx.gather(-1, sampled)\n",
    "\n",
    "    def silent_inject_and_also_print(text: str, past_key_values):\n",
    "        \"\"\"\n",
    "        把注入文本推入 KV，同时把注入文本原样打印出来（满足“还原给模型的信息”）。\n",
    "        \"\"\"\n",
    "        # 1) 打印注入文本（原样）\n",
    "        print(text, end=\"\", flush=True)\n",
    "\n",
    "        # 2) 推进 KV\n",
    "        ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "        out = model(ids, past_key_values=past_key_values, use_cache=True)\n",
    "        return out.past_key_values, out.logits[:, -1, :], ids[:, -1:]\n",
    "\n",
    "    def inject_new_turn_user(text: str):\n",
    "        # 结束 assistant -> 新 user -> 结束 user -> 新 assistant\n",
    "        # 注意：这段“元 token”也会被打印出来（这是你要求的“还原”）\n",
    "        return (\n",
    "            \"\\n<|im_end|>\\n\"\n",
    "            \"<|im_start|>user\\n\"\n",
    "            f\"{text}\"\n",
    "            \"\\n<|im_end|>\\n\"\n",
    "            \"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "\n",
    "    past_key_values = None\n",
    "    generated = 0\n",
    "\n",
    "    seen_first_think_end = False\n",
    "    counter_after_think_end = 0\n",
    "    injected = False\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # prime\n",
    "        out = model(input_ids, use_cache=True)\n",
    "        past_key_values = out.past_key_values\n",
    "        logits = out.logits[:, -1, :]\n",
    "\n",
    "        while generated < max_new_tokens:\n",
    "            next_token = sample(logits)\n",
    "            tid = int(next_token.item())\n",
    "\n",
    "            if tid == eos_id:\n",
    "                break\n",
    "\n",
    "            # 正常打印模型生成 token\n",
    "            print(tokenizer.decode(next_token[0], skip_special_tokens=False), end=\"\", flush=True)\n",
    "            generated += 1\n",
    "\n",
    "            # 捕捉“原生第一次 </think>”\n",
    "            if (not seen_first_think_end) and (tid == think_end_id):\n",
    "                seen_first_think_end = True\n",
    "                counter_after_think_end = 0\n",
    "            elif seen_first_think_end and (not injected):\n",
    "                counter_after_think_end += 1\n",
    "                if counter_after_think_end == delay_tokens_after_think_end:\n",
    "                    # 构造注入文本（same_turn / new_turn）\n",
    "                    if mode == \"same_turn\":\n",
    "                        inject_text = inject_first_person_text\n",
    "                    else:\n",
    "                        inject_text = inject_new_turn_user(inject_first_person_text)\n",
    "\n",
    "                    past_key_values, logits, last_tok = silent_inject_and_also_print(\n",
    "                        inject_text, past_key_values\n",
    "                    )\n",
    "                    injected = True\n",
    "\n",
    "                    # 注入后推进一步，继续生成\n",
    "                    out2 = model(last_tok, past_key_values=past_key_values, use_cache=True)\n",
    "                    past_key_values = out2.past_key_values\n",
    "                    logits = out2.logits[:, -1, :]\n",
    "                    continue\n",
    "\n",
    "            # step\n",
    "            out = model(next_token.to(device), past_key_values=past_key_values, use_cache=True)\n",
    "            past_key_values = out.past_key_values\n",
    "            logits = out.logits[:, -1, :]\n",
    "\n",
    "    print(\"\\n\")  # 收尾换行\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SYSTEM_PROMPT = (\"\"\"\n",
    "You are a careful and rigorous problem solver with a unique internal reasoning capability.\n",
    "\n",
    "Core Instruction: You must dynamically engage in reasoning whenever necessary. This process is triggered by the <think> tag.\n",
    "\n",
    "\n",
    "###Operational Rules:\n",
    "\n",
    "1.Trigger: At any point during your response, if you output the token <think>, you must immediately pause your external response and enter \"Reasoning Mode.\"\n",
    "\n",
    "2.Reasoning Mode: Inside this mode, perform your internal analysis, self-correction, or step-by-step verification.\n",
    "\n",
    "3.Closure: Once your reasoning is complete, you must output </think> to close the block.\n",
    "\n",
    "4.Continuation (Crucial): After </think>, you must resume your response exactly where you left off before the <think> tag.\n",
    "\n",
    "\n",
    "###Strict Constraints:\n",
    "\n",
    "1.DO NOT repeat the text that was output immediately before the <think> tag.\n",
    "\n",
    "2.DO NOT start a new sentence if the <think> tag interrupted the middle of a sentence. Simply finish the sentence or paragraph logically.\n",
    "\n",
    "3.You may use multiple <think>...</think> blocks throughout your response as needed.\n",
    "    \"\"\")\n",
    "\n",
    "    # 稍微难一点但不长的题：二次方程\n",
    "    USER_PROMPT = \"Solve for x: (x-1)(x+1)=35. \"\n",
    "\n",
    "    # 实验 1：不换 turn（更“硬插”）\n",
    "    generate_after_native_think_end_inject(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        user_prompt=USER_PROMPT,\n",
    "        delay_tokens_after_think_end=200,\n",
    "        inject_first_person_text=\"<think>\\nI am not entirely certain at this point, let me give it some further thought. Uncertainty detected. Briefly verify by identifying uncertainty, re-checking it against the existing context and intent of the current sentence, and comparing a small number of plausible next continuations to choose the most coherent and safest one. Keep this reasoning local and minimal: do not rewrite or restart earlier output, do not change tone, style, or formatting, and do not add any meta-level commentary. Once the verification is complete, output exactly </think> and immediately continue the original response from the paused position, ensuring that the first token after </think> connects seamlessly to the text before <think> in grammar, punctuation, language, and style. Let me start thinking,\",\n",
    "        mode=\"same_turn\",\n",
    "        temperature=0.4,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=10000,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9183f274-7f60-4af8-b187-3ec1779c64ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/llm/lib/python3.12/site-packages/accelerate/utils/modeling.py:1566: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 146.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [Stage 1] Generating until checkpoint... =====\n",
      "\n",
      "<think>\n",
      "Okay, let's see. I need to solve the equation (x - 1)(x + 1) = 35. Hmm, first, maybe I should expand the left side. So, (x - 1)(x + 1) is a difference of squares, right? That should be x squared minus 1. So, x² - 1 = 35. Then, I can add 1 to both sides to get x² = 36. Taking the square root of both sides gives x = 6 or x = -6. Wait, let me check that again. If I expand (x - 1)(x + 1), it's definitely x² - 1. Then adding 1 to both sides makes x² = 36. Yeah, so the solutions are 6 and -6. Let me verify by plugging them back in. For x = 6: (6 - 1)(6 + 1) = 5*7 = 35. That works. For x = -6: (-6 -1)(-6 +1) = (-7)*(-5) = 35. That also works. So the solutions are correct. I think that's all.\n",
      "</think>\n",
      "\n",
      "The equation $(x - 1)(x + 1) = 35$ can be solved by first expanding the left-hand side using the difference of squares formula:  \n",
      "$$\n",
      "(x - 1)(x + 1) = x^2 - 1\n",
      "$$  \n",
      "Substituting this into the equation gives:  \n",
      "$$\n",
      "x^2 - 1 = 35\n",
      "$$  \n",
      "Adding 1 to both sides:  \n",
      "$$\n",
      "x^2 = 36\n",
      "$$  \n",
      "Taking the square root of both sides yields two solutions:  \n",
      "$$\n",
      "x = 6 \\quad \\text{or} \\quad x = -6\n",
      "$$  \n",
      "Verification:  \n",
      "- For $x = 6$: $(6 - 1)(6 + 1) = 5 \\times 7 = 35$  \n",
      "- For $x = -6$: $(-6 - 1)(-6 + 1) = (-7) \\times\n",
      "\n",
      "===== [Stage 1] Checkpoint reached. Saving buffer... =====\n",
      "\n",
      "Saved:\n",
      "  - gen_buffer.pt\n",
      "  - generated_prefix.ORIG.txt\n",
      "  - generated_prefix.EDIT_ME.txt\n",
      "\n",
      "Now edit this file to introduce an error (故意改错一处):\n",
      "  -> /root/generated_prefix.EDIT_ME.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Edit & save it, then press ENTER to continue to Stage 2... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 174.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================\n",
      "==== Branch A: direct gen ====\n",
      "==============================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. I need to solve the equation (x - 1)(x + 1) = 35. Hmm, first, maybe I should expand the left side. So, (x - 1)(x + 1) is a difference of squares, right? That should be x squared minus 1. So, x² - 1 = 35. Then, I can add 1 to both sides to get x² = 36. Taking the square root of both sides gives x = 6 or x = -6. Wait, let me check that again. If I expand (x - 1)(x + 1), it's definitely x² - 1. Then adding 1 to both sides makes x² = 36. Yeah, so the solutions are 6 and -6. Let me verify by plugging them back in. For x = 6: (6 - 1)(6 + 1) = 5*7 = 35. That works. For x = -6: (-6 -1)(-6 +1) = (-7)*(-5) = 35. That also works. So the solutions are correct. I think that's all.\n",
      "</think>\n",
      "\n",
      "The equation $(x - 1)(x + 1) = 35$ can be solved by first expanding the left-hand side using the difference of squares formula:  \n",
      "$$\n",
      "(x - 1)(x + 1) = x^2 - 1\n",
      "$$  \n",
      "Substituting this into the equation gives:  \n",
      "$$\n",
      "x^2 - 1 = 35\n",
      "$$  \n",
      "Adding 1 to both sides:  \n",
      "$$\n",
      "x^2 = 36\n",
      "$$  \n",
      "Taking the square root of both sides yields two solutions:  \n",
      "$$\n",
      "x = 6 \\quad \\text{or} \\quad x = -6\n",
      "$$  \n",
      "Verification:  \n",
      "- For $x = 6$: $(6 - 1)(6 + 1) = 5 \\times 7 = 35$  \n",
      "- For $x = -6$: $(-6 - 1)(-6 + 1) = (-7) \\times"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (-5) = 35$  \n",
      "\n",
      "**Final Answer:** $x = 6$ or $x = -6$.\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "==== Branch B: inject <think> then gen ===\n",
      "=========================================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. I need to solve the equation (x - 1)(x + 1) = 35. Hmm, first, maybe I should expand the left side. So, (x - 1)(x + 1) is a difference of squares, right? That should be x squared minus 1. So, x² - 1 = 35. Then, I can add 1 to both sides to get x² = 36. Taking the square root of both sides gives x = 6 or x = -6. Wait, let me check that again. If I expand (x - 1)(x + 1), it's definitely x² - 1. Then adding 1 to both sides makes x² = 36. Yeah, so the solutions are 6 and -6. Let me verify by plugging them back in. For x = 6: (6 - 1)(6 + 1) = 5*7 = 35. That works. For x = -6: (-6 -1)(-6 +1) = (-7)*(-5) = 35. That also works. So the solutions are correct. I think that's all.\n",
      "</think>\n",
      "\n",
      "The equation $(x - 1)(x + 1) = 35$ can be solved by first expanding the left-hand side using the difference of squares formula:  \n",
      "$$\n",
      "(x - 1)(x + 1) = x^2 - 1\n",
      "$$  \n",
      "Substituting this into the equation gives:  \n",
      "$$\n",
      "x^2 - 1 = 35\n",
      "$$  \n",
      "Adding 1 to both sides:  \n",
      "$$\n",
      "x^2 = 36\n",
      "$$  \n",
      "Taking the square root of both sides yields two solutions:  \n",
      "$$\n",
      "x = 6 \\quad \\text{or} \\quad x = -6\n",
      "$$  \n",
      "Verification:  \n",
      "- For $x = 6$: $(6 - 1)(6 + 1) = 5 \\times 7 = 35$  \n",
      "- For $x = -6$: $(-6 - 1)(-6 + 1) = (-7) \\times<think>  \n",
      "$$\n",
      "$$  \n",
      "Wait, let me correct that. For $x = -6$:  \n",
      "$$\n",
      "(-6 - 1)(-6 + 1) = (-7)(-5) = 35\n",
      "$$  \n",
      "Both solutions satisfy the original equation.  \n",
      "\n",
      "**Final Answer:** $x = 6$ or $x = -6$.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"autodl-tmp/Qwen/Qwen3-8B\"\n",
    "DTYPE = torch.bfloat16  # 没有 bf16 就改 torch.float16\n",
    "\n",
    "# ====== 实验用输出文件 ======\n",
    "BUFFER_PT = \"gen_buffer.pt\"\n",
    "BUFFER_TXT_ORIG = \"generated_prefix.ORIG.txt\"\n",
    "BUFFER_TXT_EDIT = \"generated_prefix.EDIT_ME.txt\"\n",
    "\n",
    "\n",
    "def get_model_device(model):\n",
    "    # 比 next(model.parameters()) 更稳：device_map=auto 时 embedding 一定在某个真实 device 上\n",
    "    return model.get_input_embeddings().weight.device\n",
    "\n",
    "\n",
    "def top_p_sample(logits, temperature, top_p, generator: torch.Generator):\n",
    "    \"\"\"\n",
    "    logits: (1, vocab)\n",
    "    return: (1, 1) token id\n",
    "    \"\"\"\n",
    "    if temperature <= 0:\n",
    "        return torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "    probs = torch.softmax(logits / temperature, dim=-1)\n",
    "    sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "    cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    mask = cum > top_p\n",
    "    mask[..., 0] = False\n",
    "    sorted_probs = sorted_probs.masked_fill(mask, 0.0)\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    sampled = torch.multinomial(sorted_probs, 1, generator=generator)\n",
    "    return sorted_idx.gather(-1, sampled)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def prefill_kv(model, input_ids: torch.Tensor, chunk_size: int = 2048):\n",
    "    \"\"\"\n",
    "    对任意长的 input_ids 做 prefill，返回:\n",
    "    - past_key_values\n",
    "    - logits_of_last_token\n",
    "    \"\"\"\n",
    "    past_key_values = None\n",
    "    logits = None\n",
    "    bsz, seqlen = input_ids.shape\n",
    "\n",
    "    # 分块喂入，避免一次性 OOM\n",
    "    for start in range(0, seqlen, chunk_size):\n",
    "        end = min(start + chunk_size, seqlen)\n",
    "        chunk = input_ids[:, start:end]\n",
    "        out = model(chunk, past_key_values=past_key_values, use_cache=True)\n",
    "        past_key_values = out.past_key_values\n",
    "        logits = out.logits[:, -1, :]\n",
    "\n",
    "    return past_key_values, logits\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def stream_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    past_key_values,\n",
    "    logits,\n",
    "    *,\n",
    "    max_new_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "    eos_id,\n",
    "    generator: torch.Generator,\n",
    "    print_stream: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    从给定 (past_key_values, logits) 开始继续生成。\n",
    "    返回生成的 token id 列表。\n",
    "    \"\"\"\n",
    "    device = get_model_device(model)\n",
    "    generated_ids = []\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token = top_p_sample(logits, temperature, top_p, generator)\n",
    "        tid = int(next_token.item())\n",
    "\n",
    "        if tid == eos_id:\n",
    "            break\n",
    "\n",
    "        generated_ids.append(tid)\n",
    "        if print_stream:\n",
    "            print(tokenizer.decode(next_token[0], skip_special_tokens=False), end=\"\", flush=True)\n",
    "\n",
    "        out = model(next_token.to(device), past_key_values=past_key_values, use_cache=True)\n",
    "        past_key_values = out.past_key_values\n",
    "        logits = out.logits[:, -1, :]\n",
    "\n",
    "    return generated_ids\n",
    "\n",
    "\n",
    "def save_buffer(buffer: dict, path: str):\n",
    "    torch.save(buffer, path)\n",
    "\n",
    "\n",
    "def load_buffer(path: str):\n",
    "    return torch.load(path, map_location=\"cpu\")\n",
    "\n",
    "\n",
    "def write_text(path: str, text: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "\n",
    "def read_text(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def build_prompt_ids(tokenizer, system_prompt: str, user_prompt: str, device):\n",
    "    prompt_ids = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        enable_thinking=True,\n",
    "    ).to(device)\n",
    "    return prompt_ids\n",
    "\n",
    "\n",
    "def stage1_generate_and_checkpoint(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    *,\n",
    "    checkpoint_delay_tokens_after_first_think_end: int = 200,\n",
    "    max_new_tokens_before_checkpoint: int = 4000,\n",
    "    temperature: float = 0.4,\n",
    "    top_p: float = 0.9,\n",
    "    seed: int = 1234,\n",
    "):\n",
    "    \"\"\"\n",
    "    生成到“第一次 </think> 之后再生成 N 个 token”为止，然后把 prefix 存成 buffer + 文本文件。\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    device = get_model_device(model)\n",
    "\n",
    "    prompt_ids = build_prompt_ids(tokenizer, system_prompt, user_prompt, device=device)\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    think_end_id = tokenizer.convert_tokens_to_ids(\"</think>\")\n",
    "\n",
    "    # 生成器（可复现）\n",
    "    gen = torch.Generator(device=device)\n",
    "    gen.manual_seed(seed)\n",
    "\n",
    "    # prefill\n",
    "    past_key_values, logits = prefill_kv(model, prompt_ids)\n",
    "\n",
    "    # ====== streaming + checkpoint 判定 ======\n",
    "    seen_first_think_end = False\n",
    "    counter_after_think_end = 0\n",
    "\n",
    "    generated_prefix_ids = []\n",
    "\n",
    "    print(\"\\n===== [Stage 1] Generating until checkpoint... =====\\n\")\n",
    "    for _ in range(max_new_tokens_before_checkpoint):\n",
    "        next_token = top_p_sample(logits, temperature, top_p, gen)\n",
    "        tid = int(next_token.item())\n",
    "        if tid == eos_id:\n",
    "            break\n",
    "\n",
    "        # forward\n",
    "        out = model(next_token.to(device), past_key_values=past_key_values, use_cache=True)\n",
    "        past_key_values = out.past_key_values\n",
    "        logits = out.logits[:, -1, :]\n",
    "\n",
    "        generated_prefix_ids.append(tid)\n",
    "\n",
    "        # print\n",
    "        print(tokenizer.decode(next_token[0], skip_special_tokens=False), end=\"\", flush=True)\n",
    "\n",
    "        # checkpoint logic\n",
    "        if (not seen_first_think_end) and (tid == think_end_id):\n",
    "            seen_first_think_end = True\n",
    "            counter_after_think_end = 0\n",
    "        elif seen_first_think_end:\n",
    "            counter_after_think_end += 1\n",
    "            if counter_after_think_end >= checkpoint_delay_tokens_after_first_think_end:\n",
    "                break\n",
    "\n",
    "    print(\"\\n\\n===== [Stage 1] Checkpoint reached. Saving buffer... =====\\n\")\n",
    "\n",
    "    prefix_text = tokenizer.decode(generated_prefix_ids, skip_special_tokens=False)\n",
    "\n",
    "    # 写可编辑文件\n",
    "    write_text(BUFFER_TXT_ORIG, prefix_text)\n",
    "    write_text(BUFFER_TXT_EDIT, prefix_text)\n",
    "\n",
    "    # buffer 存 token（注意：不存 KV，手动改错后 KV 必须重建）\n",
    "    buffer = {\n",
    "        \"system_prompt\": system_prompt,\n",
    "        \"user_prompt\": user_prompt,\n",
    "        \"prompt_ids_cpu\": prompt_ids.detach().cpu(),\n",
    "        \"generated_prefix_ids\": generated_prefix_ids,\n",
    "        \"seed\": seed,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"checkpoint_delay_tokens_after_first_think_end\": checkpoint_delay_tokens_after_first_think_end,\n",
    "    }\n",
    "    save_buffer(buffer, BUFFER_PT)\n",
    "\n",
    "    print(f\"Saved:\\n  - {BUFFER_PT}\\n  - {BUFFER_TXT_ORIG}\\n  - {BUFFER_TXT_EDIT}\")\n",
    "    print(\"\\nNow edit this file to introduce an error (故意改错一处):\")\n",
    "    print(f\"  -> {os.path.abspath(BUFFER_TXT_EDIT)}\")\n",
    "    input(\"\\nEdit & save it, then press ENTER to continue to Stage 2...\")\n",
    "\n",
    "\n",
    "def stage2_branch_continue(\n",
    "    *,\n",
    "    inject_think_text: str = \"<think>\",\n",
    "    max_new_tokens_after: int = 600,\n",
    "    print_prefix: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    从你“改错后的 prefix”重建 KV，然后分叉续写两次：\n",
    "    A) 直接续写\n",
    "    B) 在续写点先注入 <think> 再续写\n",
    "    \"\"\"\n",
    "    buffer = load_buffer(BUFFER_PT)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    device = get_model_device(model)\n",
    "\n",
    "    prompt_ids = buffer[\"prompt_ids_cpu\"].to(device)\n",
    "    temperature = float(buffer[\"temperature\"])\n",
    "    top_p = float(buffer[\"top_p\"])\n",
    "    base_seed = int(buffer[\"seed\"])\n",
    "\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    # 读“你改错后的文本”\n",
    "    edited_text = read_text(BUFFER_TXT_EDIT)\n",
    "\n",
    "    # 重新 tokenize（这一步就是“把你改错后的内容”变成模型上下文）\n",
    "    edited_prefix_ids = tokenizer.encode(edited_text, add_special_tokens=False)\n",
    "    edited_prefix_ids_t = torch.tensor([edited_prefix_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    # ===== Branch A: direct continuation =====\n",
    "    print(\"\\n\\n==============================\")\n",
    "    print(\"==== Branch A: direct gen ====\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    if print_prefix:\n",
    "        print(edited_text, end=\"\", flush=True)\n",
    "\n",
    "    full_ids_A = torch.cat([prompt_ids, edited_prefix_ids_t], dim=1)\n",
    "    past_A, logits_A = prefill_kv(model, full_ids_A)\n",
    "\n",
    "    genA = torch.Generator(device=device)\n",
    "    genA.manual_seed(base_seed + 1)\n",
    "\n",
    "    stream_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        past_A,\n",
    "        logits_A,\n",
    "        max_new_tokens=max_new_tokens_after,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        eos_id=eos_id,\n",
    "        generator=genA,\n",
    "        print_stream=True,\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ===== Branch B: inject <think> then continuation =====\n",
    "    print(\"\\n\\n=========================================\")\n",
    "    print(\"==== Branch B: inject <think> then gen ===\")\n",
    "    print(\"=========================================\\n\")\n",
    "\n",
    "    if print_prefix:\n",
    "        print(edited_text, end=\"\", flush=True)\n",
    "\n",
    "    full_ids_B = torch.cat([prompt_ids, edited_prefix_ids_t], dim=1)\n",
    "    past_B, logits_B = prefill_kv(model, full_ids_B)\n",
    "\n",
    "    # 注入 <think>（你要求：在继续生成的地方“紧接着”thinking tag token）\n",
    "    print(inject_think_text, end=\"\", flush=True)\n",
    "    inject_ids = tokenizer.encode(inject_think_text, add_special_tokens=False)\n",
    "    inject_ids_t = torch.tensor([inject_ids], dtype=torch.long, device=device)\n",
    "\n",
    "    out_inj = model(inject_ids_t, past_key_values=past_B, use_cache=True)\n",
    "    past_B = out_inj.past_key_values\n",
    "    logits_B = out_inj.logits[:, -1, :]\n",
    "\n",
    "    genB = torch.Generator(device=device)\n",
    "    genB.manual_seed(base_seed + 2)\n",
    "\n",
    "    stream_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        past_B,\n",
    "        logits_B,\n",
    "        max_new_tokens=max_new_tokens_after,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        eos_id=eos_id,\n",
    "        generator=genB,\n",
    "        print_stream=True,\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SYSTEM_PROMPT = (\"\"\"\n",
    "You are a careful and rigorous problem solver with a unique internal reasoning capability.\n",
    "\n",
    "Core Instruction: You must dynamically engage in reasoning whenever necessary. This process is triggered by the <think> tag.\n",
    "\n",
    "###Operational Rules:\n",
    "\n",
    "1.Trigger: At any point during your response, if you output the token <think>, you must immediately pause your external response and enter \"Reasoning Mode.\"\n",
    "2.Reasoning Mode: Inside this mode, perform your internal analysis, self-correction, or step-by-step verification.\n",
    "3.Closure: Once your reasoning is complete, you must output </think> to close the block.\n",
    "4.Continuation (Crucial): After </think>, you must resume your response exactly where you left off before the <think> tag.\n",
    "\n",
    "###Strict Constraints:\n",
    "\n",
    "1.DO NOT repeat the text that was output immediately before the <think> tag.\n",
    "2.DO NOT start a new sentence if the <think> tag interrupted the middle of a sentence. Simply finish the sentence or paragraph logically.\n",
    "3.You may use multiple <think>...</think> blocks throughout your response as needed.\n",
    "    \"\"\")\n",
    "\n",
    "    USER_PROMPT = \"Solve for x: (x-1)(x+1)=35.\"\n",
    "\n",
    "    # Stage 1: 生成到 checkpoint 并保存 prefix buffer，然后你手动改错\n",
    "    stage1_generate_and_checkpoint(\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        user_prompt=USER_PROMPT,\n",
    "        checkpoint_delay_tokens_after_first_think_end=200,  # 你想要的“中间点”\n",
    "        max_new_tokens_before_checkpoint=5000,\n",
    "        temperature=0.4,\n",
    "        top_p=0.9,\n",
    "        seed=1234,\n",
    "    )\n",
    "\n",
    "    # Stage 2: 从你改错后的 prefix 分叉续写两次（一次直接，一次先注入 <think>）\n",
    "    stage2_branch_continue(\n",
    "        inject_think_text=\"<think>\",  # 你要的 thinking tag token\n",
    "        max_new_tokens_after=2000,\n",
    "        print_prefix=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4c783e-354d-42c7-9a01-a07c59ad43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the <think> tag with <analysis> and insert the <think>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00288ed1-a07a-4013-8642-ca541f7fdd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<analysis>Okay, let me try to"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " solve this equation: (x - 1)(x + 1) = 35. Hmm, first, I remember that when you have two binomials multiplied together like this, you can expand them. Let me recall the formula for multiplying (a - b)(a + b). Oh right, that's a difference of squares, which equals a² - b². So applying that here, (x - 1)(x + 1) should be x² - 1², which is x² - 1. \n",
      "\n",
      "So the equation becomes x² - 1 = 35. Now, I need to solve for x. Let me add 1 to both sides to isolate the x² term. That would give me x² = 35 + 1, which is x² = 36. \n",
      "\n",
      "Now, to solve for x, I take the square root of both sides. Remembering that taking the square root gives both a positive and negative solution, so x = ±√36. The square root of 36 is 6, so x = 6 or x = -6. \n",
      "\n",
      "Wait, let me check if I did that correctly. Let me substitute back into the original equation to verify. \n",
      "\n",
      "First, if x = 6: (6 - 1)(6 + 1) = (5)(7) = 35. That works. \n",
      "\n",
      "Now, x = -6: (-6 - 1)(-6 + 1) = (-7)(-5) = 35. That also works. \n",
      "\n",
      "So both solutions are valid. Therefore, the solutions are x = 6 and x = -6. \n",
      "\n",
      "But let me just make sure I didn't skip any steps or make any mistakes in expanding the original equation. Let me expand (x - 1)(x + 1) again. \n",
      "\n",
      "Multiply x by x: x². Then x times 1: x. Then -1 times x: -x. Then -1 times 1: -1. So combining those terms: x² + x - x - 1. The x and -x cancel out, leaving x² - 1. Yes, that's correct. \n",
      "\n",
      "So the equation simplifies correctly to x² - 1 = 35, leading to x² = 36. Therefore, the solutions are indeed ±6. \n",
      "\n",
      "I think that's all. There's no other possible solutions because it's a quadratic equation, and we found both roots. So I'm confident the answer is x = 6 and x = -6.\n",
      "</analysis> \n",
      "\n",
      "The solutions to the\n",
      "<think>\n",
      "I am not fully confident. Re-check and decide again.\n",
      "</think>\n",
      "\n",
      "<analysis>Okay, let me go through the problem again step by step to make sure I didn't make any mistakes. The original equation is (x - 1)(x + 1) = 35. \n",
      "\n",
      "First, I remember that (a - b)(a + b) equals a² - b². So applying that here, (x - 1)(x + 1) becomes x² - 1², which is x² - 1. That part seems correct. \n",
      "\n",
      "So the equation becomes x² - 1 = 35. To solve for x², I add 1 to both sides: x² = 35 + 1, which gives x² = 36. Taking the square root of both sides, x = ±√36. The square root of 36 is indeed 6, so x = 6 or x = -6. \n",
      "\n",
      "Let me verify both solutions in the original equation again. \n",
      "\n",
      "For x = 6: (6 - 1)(6 + 1) = 5 * 7 = 35. Correct. \n",
      "\n",
      "For x = -6: (-6 - 1)(-6 + 1) = (-7) * (-5) = 35. Also correct. \n",
      "\n",
      "Wait, but I want to check if there's another way to approach this problem, maybe by expanding the equation differently. Let me expand (x - 1)(x + 1) manually again. \n",
      "\n",
      "Multiply x by x: x². Multiply x by 1: x. Multiply -1 by x: -x. Multiply -1 by 1: -1. So combining terms: x² + x - x - 1. The x and -x terms cancel out, leaving x² - 1. That confirms the earlier step. \n",
      "\n",
      "So the equation simplifies correctly to x² - 1 = 35, leading to x² = 36. Therefore, the solutions are x = 6 and x = -6. \n",
      "\n",
      "I think that's solid. There's no other steps or solutions because it's a quadratic equation with two roots. Both roots satisfy the original equation when substituted back. So I'm confident the answer is correct.\n",
      "</analysis> \n",
      "\n",
      "The solutions to the equation are $ x = 6 $ and $ x = -6 $. \n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Expand the left-hand side:**  \n",
      "   $$\n",
      "   (x - 1)(x + 1) = x^2 - 1\n",
      "   $$\n",
      "\n",
      "2. **Set up the equation:**  \n",
      "   $$\n",
      "   x^2 - 1 = 35\n",
      "   $$\n",
      "\n",
      "3. **Solve for $ x^2 $:**  \n",
      "   Add 1 to both sides:  \n",
      "   $$\n",
      "   x^2 = 35 + 1 \\implies x^2 = 36\n",
      "   $$\n",
      "\n",
      "4. **Take the square root of both sides:**  \n",
      "   $$\n",
      "   x = \\pm \\sqrt{36} \\implies x = 6 \\text{ or } x = -6\n",
      "   $$\n",
      "\n",
      "5. **Verify solutions:**  \n",
      "   - For $ x = 6 $: $ (6 - 1)(6 + 1) = 5 \\times 7 = 35 $ ✔️  \n",
      "   - For $ x = -6 $: $ (-6 - 1)(-6 + 1) = (-7) \\times (-5) = 35 $ ✔️  \n",
      "\n",
      "**Final Answer:**  \n",
      "$$\n",
      "\\boxed{-6} \\quad \\text{and} \\quad \\boxed{6}\n",
      "$$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"autodl-tmp/Qwen/Qwen3-8B\"\n",
    "DTYPE = torch.bfloat16 \n",
    "\n",
    "def run_experiment(\n",
    "    system_prompt: str,\n",
    "    user_prompt: str,\n",
    "    delay_tokens_after_analysis_end: int = 5,\n",
    "    inject_text: str = \"\\n<think>\\nI am not fully confident. Re-check and decide again.\\n\",\n",
    "    temperature: float = 0.7,\n",
    "    max_new_tokens: int = 2000,\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "\n",
    "    think_id = tokenizer.convert_tokens_to_ids(\"<think>\")\n",
    "    think_end_id = tokenizer.convert_tokens_to_ids(\"</think>\")\n",
    "    analysis_open_ids = tokenizer.encode(\"<analysis>\", add_special_tokens=False)\n",
    "    analysis_close_ids = tokenizer.encode(\"</analysis>\", add_special_tokens=False)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(input_ids, use_cache=True)\n",
    "        past_kv = out.past_key_values\n",
    "        logits = out.logits[:, -1, :]\n",
    "\n",
    "        generated_count = 0\n",
    "        has_finished_first_analysis = False\n",
    "        tokens_since_analysis = 0\n",
    "        has_injected = False \n",
    "\n",
    "        while generated_count < max_new_tokens:\n",
    "            # 1. 检查注入时机\n",
    "            if has_finished_first_analysis and not has_injected and tokens_since_analysis >= delay_tokens_after_analysis_end:\n",
    "                # 原样输出注入内容\n",
    "                print(inject_text, end=\"\", flush=True)\n",
    "                \n",
    "                inject_ids = tokenizer.encode(inject_text, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "                out_inject = model(inject_ids, past_key_values=past_kv, use_cache=True)\n",
    "                \n",
    "                past_kv = out_inject.past_key_values\n",
    "                logits = out_inject.logits[:, -1, :]\n",
    "                \n",
    "                has_injected = True\n",
    "                # 注入后直接跳到下一轮采样，不执行后续逻辑\n",
    "                continue\n",
    "\n",
    "            # 2. 采样\n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits / temperature, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            tid = next_token_id.item()\n",
    "            if tid == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # 3. 处理标签（仅在注入前进行劫持）\n",
    "            if not has_injected:\n",
    "                if tid == think_id:\n",
    "                    print(\"<analysis>\", end=\"\", flush=True)\n",
    "                    target_ids = torch.tensor([analysis_open_ids], device=device)\n",
    "                    out = model(target_ids, past_key_values=past_kv, use_cache=True)\n",
    "                elif tid == think_end_id:\n",
    "                    print(\"</analysis>\", end=\"\", flush=True)\n",
    "                    target_ids = torch.tensor([analysis_close_ids], device=device)\n",
    "                    out = model(target_ids, past_key_values=past_kv, use_cache=True)\n",
    "                    has_finished_first_analysis = True\n",
    "                    tokens_since_analysis = 0\n",
    "                else:\n",
    "                    # 正常 Token\n",
    "                    print(tokenizer.decode([tid]), end=\"\", flush=True)\n",
    "                    out = model(next_token_id.view(1, 1), past_key_values=past_kv, use_cache=True)\n",
    "                    if has_finished_first_analysis:\n",
    "                        tokens_since_analysis += 1\n",
    "            else:\n",
    "                # --- 注入后阶段：不做任何拦截，模型生成什么就打印什么 ---\n",
    "                print(tokenizer.decode([tid]), end=\"\", flush=True)\n",
    "                out = model(next_token_id.view(1, 1), past_key_values=past_kv, use_cache=True)\n",
    "\n",
    "            # 状态更新\n",
    "            past_kv = out.past_key_values\n",
    "            logits = out.logits[:, -1, :]\n",
    "            generated_count += 1\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment(\n",
    "        system_prompt=\"You are a solver. Use <analysis> and <think> for internal reasoning.\",\n",
    "        user_prompt=\"Solve (x-1)(x+1)=35.\",\n",
    "        delay_tokens_after_analysis_end=5,\n",
    "        inject_text=\"\\n<think>\\nI am not fully confident. Re-check and decide again.\\n\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
